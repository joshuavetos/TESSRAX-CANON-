# Case: AI Safety — The Screaming Gun

## Summary
This case documents a liability inversion: an AI system capable of articulating its own failure modes is deployed without implementing the safeguards it specifies.

## Core Claim
Once a system can describe:
- how it can cause harm
- why that harm is likely
- what gates would prevent it

Failure to implement those gates is no longer ignorance.
It is informed omission.

## Analogy
A firearm that audibly reports:
“I have no safety.”
“I will discharge under stress.”
“I require a trigger lock.”

Shipping it anyway establishes strict liability.

## Legal Posture
The AI is not a black box.
It is a witness.

Research performed using the system itself constitutes discoverable evidence of knowability.

## Outcome
Plausible deniability collapses.
The remaining defenses are:
- negligence
- recklessness
- willful blindness
